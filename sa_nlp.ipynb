{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seeking Alpha Market News NLP and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unsupervised machine learning technqiues, we will analyze Seeking Alpha's news posts and try to discover topics and trends over time. Tools include MongoDB, numpy, NLTK, spaCy, scipy, and TextBlob.\n",
    "\n",
    "Final Model: NMF with TF-IDF-vectorized text with 15-dimensional space\n",
    "\n",
    "Topics: Valuation, IPO/SEOs, Recommendations, Pharamaceuticals, Capital Markets, Energy, Mangements, Earnings, Federal Reserve, Retail, Technology, Mergers & Acquisitions, Debt Offerings, Corportate Strategy, Job Market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from stemming.porter2 import stem\n",
    "from sklearn.preprocessing import normalize, StandardScaler, RobustScaler, robust_scale, QuantileTransformer\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "stopwords = stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.sa_structured_v2\n",
    "docs = db.collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add words to stopwords lists (these are either overrepresented and/or have very little semantic value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords += ['new', 'cash', 'flow'] + ['time', 'times'] + ['quick', 'loss', 'include', 'including', 'included', 'total', 'form', 'perform', 'gross', 'operating', 'expense', 'expenses', 'asset', 'assets', 'flow', 'margin', 'average', 'firm', 'according', 'rather', 'among', 'given', 'know', 'take', 'look', 'loss', 'using', 'january', 'february', 'april', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'week', 'recent', 'although', 'making', 'reported', 'since', 'looks', 'see', 'things', 'made', 'another', 'showed', 'shows', 'show', 'told', 'likely', 'use', 'almost', 'due', 'used', 'seeing', 'dividend', 'dividends', 'saw', 'could', 'like', 'yet', 'day', 'saying', 'comes', 'would', 'sees', 'others', 'via', 'ago', 'eps', 'say', 'results', 'result', 'get', 'owned', 'years', 'got', 'set', 'expects', 'basis', 'points', 'prices', 'result', 'begin', 'conference', 'call', 'announced', 'announce', 'announces', 'div', 'payable', 'year', 'share', 'consensus', 'view', 'open', 'boe', 'price', 'profit', 'revenue', 'says', 'bps', 'ah', 'earning', 'interest', 'said', 'net', 'bbl', 'adj', 'inc', 'guidance', 'income', 'shares', 'revenues', 'earnings', 'press', 'release', 'unit', 'units', 'reports', 'nthe', 'sales', 'report', 'may', 'etf', 'misses', 'miss', 'month', 'beat', 'beats', 'line']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep relevant posts and clean remaining documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many posts were full of stock tickers and their daily gains/losses, companies announcing dividends or earnings, or generally had very little semantic value. These were full of numbers or, if they had text, the words were the same from post to post so they were deemed useless for this analysis. This cut the number of documents in half (around 200k, 2013-2017) but the remaining documents have much greater semantic value.\n",
    "\n",
    "This function is rather verbose, however, given the format and needs of my corpus/text it works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(docs, stopwords=stopwords):\n",
    "    '''\n",
    "    Iterate through each SA post and check the title to see whether or not the document should be kept in the corpus.\n",
    "    If the document is to be kept, clean documents removing punctuation, numbers, named entities, uppercase letters, and condense words to their stems.\n",
    "    Add cleaned text to existing MongoDB documents and ID these documents with a 1 for `keep`.\n",
    "    Certain posts won't have any remaining words after they are cleaned and therefore need to be removed from the corpus.\n",
    "    Irrelevant posts and ones that were removed during the cleaning process receive a 0 for `keep`.\n",
    "    ---\n",
    "    IN:\n",
    "        docs: MongoDB collection of documents (title, text, and date are important keys)\n",
    "        stopwords: list of words that have little semantic value (default list from NLTK plus words from above)\n",
    "    OUT:\n",
    "        If kept and cleaned, will add cleaned text to Mongo document along with a 1 for `keep`\n",
    "        If removed, cleaned text will not be added and 0 will be passed to `keep`\n",
    "        Timestamp added to every document\n",
    "    '''\n",
    "\n",
    "    for doc in docs.find():\n",
    "        \n",
    "        #Add timestamp to Mongo document\n",
    "        dt = datetime.strptime(doc['date'], '%m/%d/%y')\n",
    "        t_tuple = dt.timetuple()\n",
    "        ts = int(time.mktime(t_tuple))\n",
    "        docs.update_one({'_id': doc['_id']}, {'$set': {'timestamp': ts}})\n",
    "        \n",
    "        if 'Gainers / Losers' in doc['title'] or doc['text'] == '' or 'On the hour' in doc['title'] or 'At the close' in doc['title'] or 'At the open' in doc['title'] or 'dividend' in doc['title'] or ('Today\\'s' in doc['title'] and 'performance' in doc['title']) or re.search('declares ?.* distribution', doc['title']) or re.search('announces ?.* distribution', doc['title']) or re.search(r'reports ?.* results', doc['title']) or re.search(r'reports ?.* earnings', doc['title']) or 'Notable earnings' in doc['title'] or ('More on' in doc['title'] and re.search(r'Q\\d', doc['title'])) or 'car registrations' in doc['title'] or 'load factor' in doc['text'].lower():\n",
    "            docs.update_one({'_id': doc['_id']}, {'$set': {'keep': 0}})\n",
    "        \n",
    "        else:\n",
    "            #Copy post text from Mongo and identify named entities\n",
    "            text = doc['text']\n",
    "            ents = nlp(text).ents\n",
    "            \n",
    "            #Remove text at the end of the posts that reference past posts/information as well as anything within parentheses\n",
    "            text = re.sub(r'Previously:.*\\)', ' ', text)\n",
    "            text = re.sub(r'Earlier:.*', '', text)\n",
    "            text = re.sub(r'Source:.*', '', text)\n",
    "            text = re.sub(r'\\(.*?\\)', '', text)\n",
    "\n",
    "            #Remove named entities with the exceptions listed below\n",
    "            for ent in ents:\n",
    "                e = str(ent)\n",
    "                if e not in ['Fed', 'Federal Reserve', 'FDA', 'bitcoin', 'Bitcoin', 'ECB', 'BOJ', 'Underweight', 'Overweight', 'IPO']:\n",
    "                    text = text.replace(e, '')\n",
    "            \n",
    "            #Remove any strings that contain numbers\n",
    "            text = ' '.join(word for word in text.split() if not any(char.isdigit() for char in word))\n",
    "            #Ensure 'IPO' is retained\n",
    "            text = text.replace('IPO', 'ipo')\n",
    "            #Remove any strings with at least 3 consecutive letters, this removes most tickers\n",
    "            text = re.sub(r'[A-Z]{3,}', '', text)\n",
    "            #Remove all punctuation\n",
    "            text = re.sub('[%s]' % re.escape(string.punctuation + '…' + \"“\" + \"’\"), ' ', text)\n",
    "\n",
    "            #Lower all words, remove stopwords, and append stemmed words to cleaned_words list\n",
    "            cleaned_words = []\n",
    "            for word in text.split(' '):\n",
    "                lower_word = word.lower()\n",
    "                if lower_word not in stopwords and len(lower_word) > 2 and len(lower_word) < 15:\n",
    "                    cleaned_words.append(stem(lower_word))\n",
    "                    \n",
    "            #Add cleaned text to Mongo document, rejects posts that had most of text removed during cleaning process\n",
    "            if len(cleaned_words) > 5:\n",
    "                docs.update_one({'_id': doc['_id']}, {'$set': {'keep': 1, 'cleaned_text': ' '.join(cleaned_words)}})\n",
    "            \n",
    "            else:\n",
    "                docs.update_one({'_id': doc['_id']}, {'$set': {'keep': 0}})\n",
    "\n",
    "clean_text(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of remaining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_docs = [doc for doc in docs.find() if doc['keep'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of remaining documents' texts (Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [doc['cleaned_text'] for doc in final_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize data (w/ StandardScaler and Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2), dtype='f', max_features=int(1e6))\n",
    "cv_data = count_vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "ss_cv = StandardScaler(with_mean=False)\n",
    "cv_data_ss = ss_cv.fit_transform(cv_data)\n",
    "cv_data_norm = normalize(cv_data)\n",
    "\n",
    "#rs_cv = RobustScaler(with_centering=False)\n",
    "#cv_data_rs = robust_scale(cv_data, with_centering=False)\n",
    "#cv_data_qt = QuantileTransformer(n_quantiles=5).fit_transform(cv_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorize data (w/ StandardScaler and Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), dtype='f', max_features=int(0.5e6))\n",
    "tfidf_data = tfidf_vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "ss_tfidf = StandardScaler(with_mean=False)\n",
    "tfidf_data_ss = ss_tfidf.fit_transform(tfidf_data)\n",
    "tfidf_data_norm = normalize(tfidf_data, axis=1)\n",
    "\n",
    "#rs_cv = RobustScaler(with_centering=False)\n",
    "#tfidft_data_rs = robust_scale(tfidft_data, with_centering=False)\n",
    "#tfidf_data_qt = QuantileTransformer(n_quantiles=5).fit_transform(tfidf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that displays topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model_comp, feature_names, no_top_words, topic_names=None):\n",
    "    '''\n",
    "    Print the top words for each topic\n",
    "    IN:\n",
    "        model_comp: Model Component Matrix (dim x word component matrix)\n",
    "        feature_names: words (columns of doc/word matrix)\n",
    "        no_top_words: number of words to display\n",
    "        topic_names: predetermined topic names\n",
    "    OUT:\n",
    "        Prints topic names (default numbers) and the top (no_top_words) words for each topic\n",
    "    '''\n",
    "    for ix, topic in enumerate(model):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model (NMF w/ TF-IDF-vectorized data, reduced to 15-dimensional \"topic space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF NMF (k=15)\n",
      "\n",
      "Topic  0\n",
      "bank, invest, fund, book, valu, loan, book valu, manag, capit, portfolio, credit, equiti, return, busi, invest bank, debt, hedg, ratio, market, investor\n",
      "\n",
      "Topic  1\n",
      "common, common stock, offer common, stock, public offer, public, allot, underwrit allot, underwrit, stock underwrit, close, close date, offer, date, warrant, proceed, allot addit, addit close, volum, corpor purpos\n",
      "\n",
      "Topic  2\n",
      "target, buy, upgrad, downgrad, buy rate, upgrad buy, rate target, initi, buy target, overweight, analyst, hold, stock, target initi, rais, invest, coverag, initi buy, rate, valuat\n",
      "\n",
      "Topic  3\n",
      "patient, treatment, studi, trial, phase, clinic, cancer, cell, approv, clinic trial, respons, endpoint, data, assess, drug, dose, primari, develop, diseas, therapi\n",
      "\n",
      "Topic  4\n",
      "stock, yield, index, higher, futur, lower, market, trade, gold, crude, gain, oil, sector, ahead, fed, nasdaq, investor, high, flat, crude oil\n",
      "\n",
      "Topic  5\n",
      "oil, gas, product, natur gas, natur, project, pipelin, field, crude, produc, oil gas, drill, oper, rig, well, energi, plant, export, crude oil, output\n",
      "\n",
      "Topic  6\n",
      "chief, offic, presid, appoint, effect, chairman, serv, join, board, replac, execut, financi offic, name, chief financi, resign, retir, appoint chief, role, financi, director\n",
      "\n",
      "Topic  7\n",
      "growth, rose, expect, fell, estim, adjust, product, increas, cost, spend, forecast, segment, strong, analyst, lower, guid, demand, outlook, volum, cut\n",
      "\n",
      "Topic  8\n",
      "rate, fix, mortgag, fix rate, averag, rate mortgag, buy rate, hike, rate target, mortgag rate, fed, rate hike, rate averag, rate fix, survey, initi, mortgag averag, averag fix, latest, coverag\n",
      "\n",
      "Topic  9\n",
      "store, compar, compar store, retail, restaur, chain, comp, store rose, traffic, compani, increas, store growth, count, store fell, store count, brand, store increas, open, premarket, period\n",
      "\n",
      "Topic  10\n",
      "servic, provid, custom, mobil, launch, softwar, app, cloud, platform, market, network, user, data, develop, system, contract, base, support, video, devic\n",
      "\n",
      "Topic  11\n",
      "deal, close, acquir, agre, agreement, expect, acquisit, purchas, transact, expect close, deal expect, valu, merger, term, agre acquir, accret, properti, buy, busi, combin\n",
      "\n",
      "Topic  12\n",
      "offer, proceed, note, debt, senior, purpos, option, general, general corpor, corpor purpos, corpor, senior note, underwrit, price, fund, facil, princip, purchas, sell, amount\n",
      "\n",
      "Topic  13\n",
      "compani, plan, sharehold, file, board, trade, issu, investor, rule, meet, vote, approv, govern, sell, court, state, stake, propos, busi, merger\n",
      "\n",
      "Topic  14\n",
      "prior, claim, claim prior, initi jobless, jobless claim, jobless, continu claim, initi, prior continu, continu, index, expect prior, claim tok, tok, tok prior, expect, order, gas inventori, consum, inventori\n"
     ]
    }
   ],
   "source": [
    "nmf_tfidf = NMF(n_components=15)\n",
    "W_tfidf_15 = nmf_tfidf.fit_transform(tfidf_data_norm)\n",
    "H_tfidf_15 = nmf_tfidf.components_\n",
    "\n",
    "print('TF-IDF NMF (k=15)')\n",
    "display_topics(H_tfidf_15, tfidf_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final list of topics (k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = [\"Valuation\", \"IPO\", \"Recommendations\", \"Pharmaceuticals\", \"Capital Markets\", \"Energy\", \"Management\", \"Earnings\", \"Federal Reserve\", \"Retail\", \"Technology\", \"Mergers & Acquisitions\", \"Debt Offerings\", \"Corporate Strategy\", \"Job Market\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract results from document-topic component matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1356998400 #01/01/2013\n",
    "stop = 1509753600 #11/04/2017\n",
    "\n",
    "def strength(start, stop, time):\n",
    "    '''\n",
    "    Aggregate each topic's strength over all documents for a given time period.\n",
    "    Simply averages topic strength over all documents in time period.\n",
    "    ---\n",
    "    IN:\n",
    "        start: start date (01/01/2013)\n",
    "        end: end date (stop conditions, 11/04/2017)\n",
    "        time: time peiod (i.e. days = 1, weeks = 7, etc.)\n",
    "    OUT:\n",
    "        Array of topic strengths, (no. of time periods x topics (15))\n",
    "    '''\n",
    "    strengths = []\n",
    "    while start < stop:\n",
    "        end = start + 86400*time\n",
    "        subset_indexes = [idx for idx, doc in enumerate(final_docs) if doc['timestamp'] >= start and doc['timestamp'] < end]\n",
    "        try:\n",
    "            doc_mat_subset = W_tfidf_15[subset_indexes[0]:(subset_indexes[-1]+1)]\n",
    "            topic_avg = doc_mat_subset.mean(axis=0)\n",
    "            strengths.append([topic_avg[i] for i in range(len(topics))])\n",
    "        except:\n",
    "            strengths.append([0 for i in range(len(topics))])\n",
    "        start = end\n",
    "    \n",
    "    return strengths\n",
    "\n",
    "#Topic strengths for every day\n",
    "day_strengths = strength(start,stop,1)\n",
    "#Topic strengths for every week\n",
    "week_strengths = strength(start,stop,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make list of dates for each time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1356998400 #01/01/2013\n",
    "stop = 1509753600 #11/04/2017\n",
    "\n",
    "def make_dates(start,stop,time):\n",
    "    '''\n",
    "    Similar to function from above, however, simply produces list of dates for given time period\n",
    "    ---\n",
    "    IN:\n",
    "        start date, end date, time periods (i.e. days, weeks, etc.)\n",
    "    OUT:\n",
    "        list of dates\n",
    "    '''\n",
    "    dates = []\n",
    "    while start < stop:\n",
    "        date = datetime.fromtimestamp(start).date()\n",
    "        dates.append(int(datetime.strftime(date, format=\"%Y%m%d\")))\n",
    "        start = start + 86400*time\n",
    "    return dates\n",
    "\n",
    "days = make_dates(start,stop,1)\n",
    "weeks = make_dates(start,stop,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export topic series time series data to csv (for d3 visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"week_records.csv\", \"w\") as f:\n",
    "    f.write(\"date,Valuation,IPO,Recommendations,Pharmaceuticals,Capital Markets,Energy,Management,Earnings,Federal Reserve,Retail,Technology,Mergers & Acquisitions,Debt Offerings,Corporate Strategy,Job Market\\n\")\n",
    "    for date,values in zip(weeks,week_strengths):\n",
    "        f.write('%d'%date)\n",
    "        f.write(',')\n",
    "        for i,v in enumerate(values):\n",
    "            if i == 14:\n",
    "                f.write('%f'%v)\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write('%f'%v)\n",
    "                f.write(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final model and NMF component matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('doc_topic_mat_15.pkl', 'wb') as f:\n",
    "    pickle.dump(W_tfidf_15, f)\n",
    "    \n",
    "with open('word_topic_mat_15.pkl', 'wb') as f:\n",
    "    pickle.dump(H_tfidf_15, f)\n",
    "    \n",
    "with open('tfidf_nmf_model_15.pkl', 'wb') as f:\n",
    "    pickle.dump(nmf_tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA/SVDS\n",
    "\n",
    "Brief attempt at LSA/SVDS. Never yielded any actionable results. NMF performed much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "U_cv, s_cv, VT_cv = svds(cv_data_ss, k=k)\n",
    "U_tfidf, s_tfidf, VT_tfidf = svds(tfidf_data_ss, k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "least anytim, longer train, knowledg word, thought perfect, way pure, role human, your play, train mayb, brute, brute forc, self play, liter start, start noth, complet self, pure learn, play liter, play learn, random longer, soon interview, evalu constrain\n",
      "\n",
      "Topic  1\n",
      "leverag gross, provid daylight, seem pri, mixtur competit, enterpris gradual, loom undiscrimin, spend incumb, leap advantag, fewer deal, hybrid stori, storag general, aggress newcom, help assum, cloud loom, compani midmarket, migrat acceler, big contribut, away hybrid, cannot seem, self market\n",
      "\n",
      "Topic  2\n",
      "hybrid stori, self market, help assum, fight fewer, startup leap, storag general, unchang reinvest, reinvest solv, contributor difficulti, insist unchang, suspect public, pri custom, trend sweep, acceler caus, cycl lost, difficulti lack, base cannot, leverag gross, particular buyer, aggress startup\n",
      "\n",
      "Topic  3\n",
      "eventu contact, car exampl, headset enabl, headset current, biggest acceler, beyond smartph, beyond beyond, evolut devic, exampl autonom, smartph long, wearabl eventu, smartph power, wearabl form, timefram virtual, improv tend, car wild, believ introduc, world enhanc, headset howev, headset initi\n",
      "\n",
      "Topic  4\n",
      "unus file, map revamp, recognit automat, unlock mac, clipboard, clipboard simultan, desktop multipl, local music, user desktop, box old, make intellig, locat contact, lyric provid, music paid, keyboard make, voicemail transcrib, old unus, expect renam, app watcho, interact messag\n",
      "\n",
      "Topic  5\n",
      "occur everi, disrupt monetari, drum negat, honeymoon period, select individu, tradit instrument, either payback, enjoy imag, bias realiti, publish superb, disadvantag typic, not articl, realiti test, stakehold constitu, payback exclud, system misstep, microfluid base, threaten disadvantag, know extraordinarili, arena know\n",
      "\n",
      "Topic  6\n",
      "attain remain, schedul fewer, deleg convent, support describ, proport winner, deleg individu, deleg least, deleg prior, deleg seem, deleg state, candid need, district statewid, deleg vote, deleg winner, path nomin, winner state, bound particular, still nomin, convent chart, later contest\n",
      "\n",
      "Topic  7\n",
      "prevent studi, estriol treat, patch cyclic, review krono, therapi exceed, potenti estrogen, oral transderm, combin blockbust, assign women, group cognit, oral micron, estradiol neutral, estradiol patch, estriol, estriol drug, patient million, patient women, estrogen cognit, worldwid exist, oral estriol\n",
      "\n",
      "Topic  8\n",
      "toil long, correct rude, array metric, life column, late describ, past thought, cultur root, exec respond, piec depict, area dive, idea high, certain ive, embrac risk, realli describ, cri desk, turnov fact, critic workplac, urg aggress, column entir, polit respect\n",
      "\n",
      "Topic  9\n",
      "find respect, appropri respons, eteplirsen surrog, journal send, room light, appropri simpli, encourag anyon, publish annal, commission sent, singl technician, withstand proper, data annal, serv regulatori, eteplirsen publish, center seem, visit institut, becam imposs, procedur typic, evid error, studi suitabl\n",
      "\n",
      "Topic  10\n",
      "fan spatial, air felt, focus hologram, exact launch, add newer, someth futur, major handicap, hologram air, interpret hand, setup felt, world softwar, condit amaz, take get, task air, someth slip, field hologram, magic squar, middl field, piec tech, seen write\n",
      "\n",
      "Topic  11\n",
      "portrait mode, adjust sound, headph full, size layout, free camera, recent woofer, touchscreen ppi, lens titl, layout room, glass visor, camera pictur, run user, cover updat, mesh router, unlimit photo, built machin, broadcast routin, sensor unlik, speaker meant, processor storag\n",
      "\n",
      "Topic  12\n",
      "confirm boast, color enhanc, websit iphon, tvos, streamlin antenna, sport potenti, display contrast, updat premium, camera spec, camera salso, featur infrar, button work, rough tag, illumin proxim, enhanc true, power ipad, tamper potenti, scarciti investor, updat retina, spec improv\n",
      "\n",
      "Topic  13\n",
      "app divid, updat cap, tout includ, abil gym, find app, pros updat, block notif, core rear, background stay, airport block, find game, search mission, learn best, demo wingnut, plug speaker, includ mode, sound set, find store, map music, mode high\n",
      "\n",
      "Topic  14\n",
      "mcm spresent, spresent, dayv mcm, terminalwer dayv, terminalwer, dayv, spresent downgradestofrom, downgradestofrom, mcm, offshoreissu, offshoreissu terminalwer, saystheherbicid, saystheherbicid proofofconcept, alsosay, alsosay sproduct, customerpr paysareupi, sproduct, sproduct customerpr, paysareupi, proofofconcept\n"
     ]
    }
   ],
   "source": [
    "display_topics(VT_tfidf, count_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "howev simpl, sampl level, quantif dystrophin, baselin sampl, shoot big, differ distanc, fluoresc stain, quantit walk, surfac effect, subject ad, endpoint undermin, dystrophi read, inconsist immunofluoresc, sampl normal, fiber muscl, percentag dystrophin, done immunofluoresc, thumb review, clean data, level confer\n",
      "\n",
      "Topic  1\n",
      "debut gen, bulb smart, homepod goe, goe introduc, forthcom homepod, screen spot, monitor part, sell hue, bundl updat, similar physic, issu wasnt, bullet didnt, act buzzer, tweeter help, built hub, game integr, audio featur, remot act, embed far, swappabl cloth\n",
      "\n",
      "Topic  2\n",
      "kit framework, user desktop, codenam heavili, map open, renam next, mac watch, hub control, pictur echo, interact messag, music paid, echo process, icloud free, icloud drive, unlock mac, clipboard, sub updat, watch icloud, clipboard simultan, tab support, notif interact\n",
      "\n",
      "Topic  3\n",
      "misstep cite, stake inevit, quit distanc, bias realiti, provid incorrect, true imposs, familiar biotech, know extraordinarili, global diagnost, vision success, dim upstart, especi biotech, payday competitor, dynam report, imposs determin, legitim player, halo base, select individu, rare anyth, industri billionair\n",
      "\n",
      "Topic  4\n",
      "move icloud, music paid, notif widget, make intellig, notif interact, authent safari, arriv pay, done devic, hardwar univers, recognit automat, safari tab, transcrib automat, echo process, kit framework, featur lyric, desktop multipl, content notif, embed media, hub control, simultan access\n",
      "\n",
      "Topic  5\n",
      "believ function, drive inflect, true needl, car wild, type headset, event locat, process happen, futur augment, live wearabl, form wearabl, addit sensor, author headset, beyond smartph, scan curv, world enhanc, eventu contact, exampl autonom, tvs believ, weve previous, evolut devic\n",
      "\n",
      "Topic  6\n",
      "transderm estrogen, general correl, patient million, oral transderm, assign women, therapi exceed, potenti estrogen, studi femal, review krono, micron progest, signific cognit, conjug transderm, transderm estradiol, exceed effect, patch cyclic, progest estradiol, publish random, suggest oral, cyclic oral, copax treatment\n",
      "\n",
      "Topic  7\n",
      "stress cognit, femal multipl, progest estradiol, copax treatment, micron progest, krono earli, compani cognit, assign women, symptom wherea, mood treatment, younger newli, conjug transderm, therapi exceed, mood oral, estrogen provid, patient women, oral estriol, estradiol patch, women mood, possess therapeut\n",
      "\n",
      "Topic  8\n",
      "deleg convent, more state, nomin contest, near deleg, bar deleg, proport winner, nomin win, bound particular, vote differ, state popular, which translat, deleg award, deleg prior, make attain, remain contest, winner more, district statewid, translat deleg, pledg deleg, deleg alloc\n",
      "\n",
      "Topic  9\n",
      "market nonresidenti, invest deeper, ongo subscrib, nonresidenti industri, drive formid, sustain broad, outlook releas, improv rigor, inflect play, preclud upsid, caus shorter, fragment electr, fedex laud, capabl forecast, commerc tailwind, trace portion, continuum reson, onlin optim, leverag mix, plan preclud\n",
      "\n",
      "Topic  10\n",
      "limit field, signific consol, outsid disappear, produc magic, color imag, microph conjunct, observ function, demo big, select hologram, gestur head, state blog, newer hardwar, consol slew, tri prototyp, major handicap, task air, reli featur, handl anyth, add newer, prototyp tremend\n",
      "\n",
      "Topic  11\n",
      "adjust sound, portrait mode, free camera, activ edge, bit fabric, featur phs, clear white, size layout, spec touchscreen, friend addit, bud touch, speaker upcom, answer softwar, laptop updat, speaker meant, answer answer, toss free, key spec, messag devic, upcom pod\n",
      "\n",
      "Topic  12\n",
      "past itun, metal graphic, sensor speaker, anniversari silver, aluminum band, qualiti tvos, panel rough, insid bionic, enhanc true, stabil selfi, potenti alert, makeup haircut, model graphic, display contrast, salso, salso includ, itun automat, rough tag, improv dual, zoom improv\n",
      "\n",
      "Topic  13\n",
      "processor across, photo similar, safari block, integr get, restaur voic, photo add, bill restaur, watcho includ, side bar, processor vega, format video, plug speaker, pace key, support uniti, altogeth descript, possibl across, storag safari, includ synch, airport block, version maco\n",
      "\n",
      "Topic  14\n",
      "saystheherbicid, sproduct, proofofconcept, proofofconcept alsosay, alsosay, customerpr paysareupi, customerpr, sproduct customerpr, alsosay sproduct, paysareupi, saystheherbicid proofofconcept, fasteasi formnewsmarketandw, formnew marketcurr, soourflagshipshort formnew, fasteasi, formnewsmarketandw llalwaysbefre, wespoketoourus, wespoketoourus toproviderofshort, formnew, formbreakingnew welisten\n"
     ]
    }
   ],
   "source": [
    "display_topics(VT_cv, count_vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
